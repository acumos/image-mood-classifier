#!/usr/bin/env python3
import connexion
import logging

import argparse
import json
import time

from flask import Flask, request, current_app, make_response

import pandas as pd
import requests

import sys
if sys.version_info[0] < 3:
    from StringIO import StringIO
else:
    from io import StringIO

from cognita_client.wrap.load import load_model
from image_mood_classifier.prediction_formatter import Formatter

def generate_output(pred, rich_output, time_ellapse):
    if rich_output:
        # NOTE: This response is specially formatted for the webdemo included with this package.
        #       Alternate forms of a response are viable for any other desired application.
        retObj = {
            'classes': [],
            'clientfile': 'undefined',
            'info': 'Processed',
            'processingtime': time_ellapse,
            'serverfilename': '/dev/null',
            'status': 'Succeeded'
        }

        # iterate through predictions
        for r in zip(pred[Formatter.COL_NAME_CLASS], pred[Formatter.COL_NAME_PREDICTION], range(len(pred))):
            retObj['classes'].append({Formatter.COL_NAME_CLASS:r[0], 'rank':r[2], Formatter.COL_NAME_PREDICTION:r[1], Formatter.COL_NAME_IDX:0 })

        # dump to pretty JSON
        retStr = json.dumps({'results':retObj}, indent=4)
    else:
        retStr = json.dumps(pred.to_dict(orient='records'), indent=4)

    # formulate response
    resp = make_response((retStr, 200, { } ))
    # allow 'localhost' from 'file' or other;
    # NOTE: DO NOT USE IN PRODUCTION!!!
    resp.headers['Access-Control-Allow-Origin'] = '*'
    print(type(pred))
    print(retStr[:min(200,len(retStr))])
    #print(pred)
    return resp

def transform_features(class_predictions, rich_output=False):
    app = current_app
    time_start = time.clock()
    str_predictions = class_predictions.stream.read().decode()

    # first pass is attempting to parse CSV
    try:
        objJson = json.loads(str_predictions)
        X = pd.DataFrame(objJson)
    except ValueError as e:
        class_predictions = StringIO(str_predictions)
        X = pd.read_csv(class_predictions)
    #print(X)

    pred = app.model.transform.from_native(X).as_native()
    time_stop = time.clock()
    return generate_output(pred, (time_stop - time_start), rich_output)


#def invoke_method(model_method):
def transform(mime_type, image_binary, rich_output=False, native_transform=False):
    app = current_app
    if app.model_image is None:
        # formulate response
        resp = make_response(("Image classification model not loaded, image-based transform not possible", 500, {}))
        resp.headers['Access-Control-Allow-Origin'] = '*'
        return resp

    time_start = time.clock()
    image_read = image_binary.stream.read()
    X = pd.DataFrame([['image/jpeg', image_read]], columns=['mime_type', 'binary_stream'])

    # note that we keep it in proto format, by not transforming back to native
    predImage_out = app.model_image.transform.from_native(X)
    if native_transform:       # for regression testing
        print(predImage_out.as_native())
        predDf = app.model.transform.from_native(predImage_out.as_native()).as_native()
    else:
        # final transform DOES use native format as last output,j ust as in python-client/testing/wrap/runner.py example
        predDf = app.model.transform.from_msg(predImage_out.as_msg()).as_native()
    time_stop = time.clock()
    return generate_output(predDf, (time_stop - time_start), rich_output)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=8886, help='port to launch the simple web server')
    parser.add_argument("--modeldir", type=str, default='../model', help='model directory to load dumped image-mood-classifier')
    parser.add_argument("--modeldir_image", type=str, default='', help='model directory for image classfier')
    pargs = parser.parse_args()

    print("Configuring local application... {:}".format(__name__))
    logging.basicConfig(level=logging.INFO)
    app = connexion.App(__name__)
    app.add_api('swagger.yaml')
    # example usage:
    #     curl -F image_binary=@test.jpg -F mime_type="image/jpeg" "http://localhost:8885/transform"

    print("Loading model... {:}".format(pargs.modeldir))
    app.app.model = load_model(pargs.modeldir)  # refers to ./model dir in pwd. generated by helper script also in this dir

    app.app.model_image = None
    if pargs.modeldir_image:
        print("Loading image classifier model... {:}".format(pargs.modeldir_image))
        app.app.model_image = load_model(pargs.modeldir_image)  # refers to ./model dir in pwd. generated by helper script also in this dir

    # # dynamically add handlers depending on model capabilities
    # for method_name, method in model.methods.items():
    #     url = "/{}".format(method_name)
    #     print("Adding route {}".format(url))
    #     handler = partial(invoke_method, model_method=method)
    #     app.add_url_rule(url, method_name, handler, methods=['POST'])

    # run our standalone gevent server
    app.run(port=pargs.port) #, server='gevent')
